{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is CUDA available for PyTorch:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wtp: datasets.DatasetDict = datasets.load_dataset(\"YawKar/wikitext_with_entitled_paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cook_gpt2_tokenizer():\n",
    "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cook_summarizer() -> transformers.Pipeline:\n",
    "    return transformers.pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_wiki_wtp(\n",
    "    wiki_wtp: datasets.DatasetDict,\n",
    "    tokenizer: transformers.tokenization_utils.PreTrainedTokenizerBase,\n",
    "    summarizer: transformers.Pipeline,\n",
    "    summarizer_max_tokens: int,\n",
    "    max_tokens_length: int,\n",
    ") -> datasets.DatasetDict:\n",
    "    if max_tokens_length <= 0:\n",
    "        raise Exception(f\"max_tokens_length isn't positive: {max_tokens_length}\")\n",
    "\n",
    "    def batch_concat_with_summarization(batch: dict[str, list[str]]) -> dict:\n",
    "        processed = {\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "        }\n",
    "        for heading, paragraph in zip(batch[\"heading\"], batch[\"paragraph\"]):\n",
    "            concat = heading + paragraph\n",
    "            tokenized = tokenizer(concat)\n",
    "            if len(tokenized[\"input_ids\"]) > max_tokens_length:\n",
    "                cumulative_tokens = 0\n",
    "                batch_to_summarize: list[str] = []\n",
    "                summarized: list[str] = []\n",
    "                for sentence, tokens in [\n",
    "                    (sentence, len(summarizer.tokenizer(sentence + \".\")))\n",
    "                    for sentence in paragraph.split(\".\")\n",
    "                ]:\n",
    "                    if cumulative_tokens + tokens > summarizer_max_tokens:\n",
    "                        # summarize batch\n",
    "                        summarized.append(\n",
    "                            summarizer(\n",
    "                                \".\".join(batch_to_summarize),\n",
    "                                min_length=0,\n",
    "                                max_length=cumulative_tokens - 1,\n",
    "                                truncation=True,\n",
    "                            )[0][\"summary_text\"]\n",
    "                        )\n",
    "                        cumulative_tokens = 0\n",
    "                        batch_to_summarize.clear()\n",
    "                    cumulative_tokens += tokens\n",
    "                    batch_to_summarize.append(sentence)\n",
    "\n",
    "                # summarize the last batch\n",
    "                summarized.append(\n",
    "                    summarizer(\n",
    "                        \".\".join(batch_to_summarize),\n",
    "                        min_length=0,\n",
    "                        max_length=cumulative_tokens - 1,\n",
    "                    )[0][\"summary_text\"]\n",
    "                )\n",
    "                cumulative_tokens = 0\n",
    "                batch_to_summarize.clear()\n",
    "\n",
    "                concat: str = heading + \".\".join(summarized)\n",
    "                tokenized = tokenizer(concat)\n",
    "            processed[\"input_ids\"].append(tokenized[\"input_ids\"][:max_tokens_length])\n",
    "            processed[\"attention_mask\"].append(\n",
    "                tokenized[\"attention_mask\"][:max_tokens_length]\n",
    "            )\n",
    "        return processed\n",
    "\n",
    "    return wiki_wtp.map(batch_concat_with_summarization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_wiki_wtp = preprocess_wiki_wtp(wiki_wtp, cook_gpt2_tokenizer(), cook_summarizer(), 1024, 1022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_wiki_wtp.push_to_hub(\"summarized_and_tokenized_by_gpt2_wiki\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
